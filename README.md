# ğŸ§  NeuroNerd

A fine-tuned Llama 3.1 8B model specialized in **neuroscience and cognitive science**. Built with a custom data pipeline and trained using Unsloth.

## ğŸ”— Model

**HuggingFace:** [VickyK09/neuronerd-llama-8b](https://huggingface.co/VickyK09/neuronerd-llama-8b)

## ğŸ“ Project Structure

```
neuronerd/
â”œâ”€â”€ app.py                  # Streamlit chatbot for inference
â”œâ”€â”€ config.py               # Pipeline configuration
â”œâ”€â”€ run_pipeline.py         # Data generation orchestrator
â”œâ”€â”€ requirements.txt        # Dependencies
â”œâ”€â”€ scripts/                # Data pipeline scripts
â”‚   â”œâ”€â”€ 01_extract_text.py  # PDF text extraction
â”‚   â”œâ”€â”€ 02_preprocess.py    # Text cleaning & chunking
â”‚   â”œâ”€â”€ 03_generate_qa.py   # Q&A generation (Gemini)
â”‚   â”œâ”€â”€ 04_quality_control.py # Data filtering
â”‚   â””â”€â”€ 05_finalize.py      # Train/test split
â”œâ”€â”€ training/               # Model training
â”‚   â””â”€â”€ train_runpod.py     # Unsloth training script (RunPod)
â”œâ”€â”€ pdfs/                   # Source textbooks
â””â”€â”€ output/                 # Generated data
    â””â”€â”€ final/              # train.jsonl & test.jsonl
```

## ğŸš€ Quick Start

### Run the Chatbot

```bash
pip install -r requirements.txt
streamlit run app.py
```

### Generate Dataset (from PDFs)

```bash
# Set your Gemini API key
echo "GOOGLE_API_KEY=your_key_here" > .env

# Run the full pipeline
python run_pipeline.py --all
```

### Train on RunPod

1. Upload to GitHub
2. Create a RunPod pod with `unslothai/unsloth:latest` Docker image
3. Clone repo and run:
```bash
python training/train_runpod.py
```

## ğŸ“š Training Data Sources

- *Computational Exploration in Cognitive Neuroscience*
- *Foundations of Neuroscience* (Casey Henley)
- *The Cognitive Neurosciences* (Gazzaniga et al.)

## ğŸ› ï¸ Tech Stack

| Component | Technology |
|-----------|------------|
| Data Generation | Gemini 2.5 Flash Lite |
| Training | Unsloth, QLoRA, HuggingFace TRL |
| Infrastructure | RunPod (A100 GPU) |
| Inference | Streamlit, Transformers |
| Model | Llama 3.1 8B |

## ğŸ“Š Stats

- **Dataset:** 8,200+ Q&A pairs
- **Training Time:** ~25 minutes (A100)
- **Total Cost:** < $3

## ğŸ“„ License

Model released under the [Llama 3.1 Community License](https://github.com/meta-llama/llama-models/blob/main/models/llama3_1/LICENSE).

---

*Built with â¤ï¸ using [Unsloth](https://github.com/unslothai/unsloth)*
